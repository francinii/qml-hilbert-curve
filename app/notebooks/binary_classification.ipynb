{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe08cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running QUANTUM QuantumClassifier ---\n",
      "Distribution of classes in training data (y_train):\n",
      "  Class 0: 1998 samples\n",
      "  Class 1: 2000 samples\n",
      "Distribution of classes in test data (y_test):\n",
      "  Class 0: 249 samples\n",
      "  Class 1: 250 samples\n",
      "--------------- EPOCHS --------------------\n",
      "Average loss over epoch 1: 0.6959\n",
      "Accuracy: 0.4990 - F1: 0.0000 - Recall: 0.0000\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 2: 0.6925\n",
      "Accuracy: 0.5892 - F1: 0.4092 - Recall: 0.2840\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 3: 0.6917\n",
      "Accuracy: 0.6092 - F1: 0.5324 - Recall: 0.4440\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 4: 0.6912\n",
      "Accuracy: 0.5371 - F1: 0.6647 - Recall: 0.9160\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 5: 0.6907\n",
      "Accuracy: 0.6232 - F1: 0.6163 - Recall: 0.6040\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 6: 0.6901\n",
      "Accuracy: 0.6192 - F1: 0.4920 - Recall: 0.3680\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 7: 0.6894\n",
      "Accuracy: 0.6393 - F1: 0.6250 - Recall: 0.6000\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 8: 0.6886\n",
      "Accuracy: 0.6032 - F1: 0.6722 - Recall: 0.8120\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 9: 0.6877\n",
      "Accuracy: 0.6493 - F1: 0.6421 - Recall: 0.6280\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 10: 0.6865\n",
      "Accuracy: 0.6453 - F1: 0.5586 - Recall: 0.4480\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 11: 0.6850\n",
      "Accuracy: 0.6433 - F1: 0.5412 - Recall: 0.4200\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 12: 0.6833\n",
      "Accuracy: 0.6573 - F1: 0.6460 - Recall: 0.6240\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 13: 0.6810\n",
      "Accuracy: 0.6513 - F1: 0.5561 - Recall: 0.4360\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 14: 0.6779\n",
      "Accuracy: 0.6192 - F1: 0.6747 - Recall: 0.7880\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 15: 0.6746\n",
      "Accuracy: 0.6874 - F1: 0.6518 - Recall: 0.5840\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 16: 0.6695\n",
      "Accuracy: 0.6814 - F1: 0.6638 - Recall: 0.6280\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 17: 0.6640\n",
      "Accuracy: 0.6854 - F1: 0.6440 - Recall: 0.5680\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 18: 0.6560\n",
      "Accuracy: 0.6914 - F1: 0.6638 - Recall: 0.6080\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 19: 0.6473\n",
      "Accuracy: 0.6653 - F1: 0.6902 - Recall: 0.7440\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 20: 0.6366\n",
      "Accuracy: 0.6934 - F1: 0.6909 - Recall: 0.6840\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 21: 0.6269\n",
      "Accuracy: 0.6994 - F1: 0.6875 - Recall: 0.6600\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 22: 0.6189\n",
      "Accuracy: 0.7054 - F1: 0.6682 - Recall: 0.5920\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 23: 0.6114\n",
      "Accuracy: 0.7034 - F1: 0.6992 - Recall: 0.6880\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 24: 0.6030\n",
      "Accuracy: 0.7034 - F1: 0.6542 - Recall: 0.5600\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 25: 0.6003\n",
      "Accuracy: 0.6934 - F1: 0.7140 - Recall: 0.7640\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 26: 0.5973\n",
      "Accuracy: 0.7234 - F1: 0.7101 - Recall: 0.6760\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 27: 0.5947\n",
      "Accuracy: 0.7014 - F1: 0.7107 - Recall: 0.7320\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 28: 0.5940\n",
      "Accuracy: 0.7154 - F1: 0.7149 - Recall: 0.7120\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 29: 0.5927\n",
      "Accuracy: 0.7174 - F1: 0.7140 - Recall: 0.7040\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 30: 0.5913\n",
      "Accuracy: 0.6754 - F1: 0.7065 - Recall: 0.7800\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 31: 0.5924\n",
      "Accuracy: 0.7154 - F1: 0.7171 - Recall: 0.7200\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 32: 0.5923\n",
      "Accuracy: 0.7355 - F1: 0.7093 - Recall: 0.6440\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 33: 0.5908\n",
      "Accuracy: 0.6112 - F1: 0.6911 - Recall: 0.8680\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 34: 0.5910\n",
      "Accuracy: 0.7315 - F1: 0.6884 - Recall: 0.5920\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 35: 0.5868\n",
      "Accuracy: 0.6774 - F1: 0.7024 - Recall: 0.7600\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 36: 0.5889\n",
      "Accuracy: 0.7415 - F1: 0.7034 - Recall: 0.6120\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 37: 0.5902\n",
      "Accuracy: 0.7194 - F1: 0.7034 - Recall: 0.6640\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 38: 0.5883\n",
      "Accuracy: 0.7295 - F1: 0.7007 - Recall: 0.6320\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 39: 0.5901\n",
      "Accuracy: 0.7234 - F1: 0.6618 - Recall: 0.5400\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 40: 0.5868\n",
      "Accuracy: 0.7255 - F1: 0.6989 - Recall: 0.6360\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 41: 0.5893\n",
      "Accuracy: 0.7335 - F1: 0.7038 - Recall: 0.6320\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 42: 0.5901\n",
      "Accuracy: 0.7214 - F1: 0.7086 - Recall: 0.6760\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 43: 0.5862\n",
      "Accuracy: 0.7174 - F1: 0.7152 - Recall: 0.7080\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 44: 0.5856\n",
      "Accuracy: 0.7214 - F1: 0.7086 - Recall: 0.6760\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 45: 0.5842\n",
      "Accuracy: 0.7435 - F1: 0.7078 - Recall: 0.6200\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 46: 0.5855\n",
      "Accuracy: 0.7335 - F1: 0.7025 - Recall: 0.6280\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 47: 0.5850\n",
      "Accuracy: 0.7134 - F1: 0.7111 - Recall: 0.7040\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 48: 0.5842\n",
      "Accuracy: 0.7255 - F1: 0.7066 - Recall: 0.6600\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 49: 0.5881\n",
      "Accuracy: 0.6774 - F1: 0.6968 - Recall: 0.7400\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 50: 0.5842\n",
      "Accuracy: 0.7114 - F1: 0.7037 - Recall: 0.6840\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 51: 0.5860\n",
      "Accuracy: 0.7114 - F1: 0.7154 - Recall: 0.7240\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 52: 0.5872\n",
      "Accuracy: 0.7214 - F1: 0.7011 - Recall: 0.6520\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 53: 0.5849\n",
      "Accuracy: 0.7114 - F1: 0.7143 - Recall: 0.7200\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 54: 0.5847\n",
      "Accuracy: 0.7234 - F1: 0.7089 - Recall: 0.6720\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 55: 0.5858\n",
      "Accuracy: 0.7114 - F1: 0.7049 - Recall: 0.6880\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 56: 0.5840\n",
      "Accuracy: 0.7114 - F1: 0.7143 - Recall: 0.7200\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 57: 0.5850\n",
      "Accuracy: 0.6633 - F1: 0.6978 - Recall: 0.7760\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 58: 0.5835\n",
      "Accuracy: 0.6613 - F1: 0.6988 - Recall: 0.7840\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 59: 0.5839\n",
      "Accuracy: 0.7315 - F1: 0.7009 - Recall: 0.6280\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 60: 0.5830\n",
      "Accuracy: 0.7435 - F1: 0.6938 - Recall: 0.5800\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 61: 0.5815\n",
      "Accuracy: 0.7094 - F1: 0.7035 - Recall: 0.6880\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 62: 0.5825\n",
      "Accuracy: 0.7275 - F1: 0.7094 - Recall: 0.6640\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 63: 0.5812\n",
      "Accuracy: 0.7375 - F1: 0.7029 - Recall: 0.6200\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 64: 0.5802\n",
      "Accuracy: 0.7335 - F1: 0.7038 - Recall: 0.6320\n",
      "---------------------------------------------------\n",
      "Average loss over epoch 65: 0.5834\n",
      "Accuracy: 0.7295 - F1: 0.7007 - Recall: 0.6320\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, recall_score, f1_score\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import random\n",
    "import joblib # Import joblib for saving/loading sklearn objects\n",
    "\n",
    "# Constants\n",
    "CLASS_MAP = {\n",
    "    \"tumor\": 0,\n",
    "    \"no_tumor\": 1\n",
    "}\n",
    "CLASS_MAP_NUMBER = len(CLASS_MAP)\n",
    "BASE_URL= \"../data/dataset_binary/\"\n",
    "TRAINING_URL= BASE_URL + \"Training/\"\n",
    "TESTING_URL= BASE_URL + \"Testing/\"\n",
    "RESULTS_GRAPHICS_URL = '../results/graphics/'\n",
    "RESULTS_CSV_URL = '../results/csv/'\n",
    "RESULTS_MODELS_URL = '../models'\n",
    "RESULTS_PREPROCESSING_URL = '../models'\n",
    "\n",
    "class PlotUtils:\n",
    "    @staticmethod\n",
    "    def plot_loss(loss_history, title='Loss function by epochs', save_path=None):\n",
    "        plt.figure()\n",
    "        plt.plot(loss_history, marker='o')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_loss_vs_accuracy(loss_history, accuracy_history, title='Loss vs Accuracy by Epoch', save_path=None):\n",
    "        plt.figure()\n",
    "        plt.plot(loss_history, label='Loss', color='red')\n",
    "        plt.plot(accuracy_history, label='Accuracy', color='blue')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(cm, class_names=None, title='Confusion Matrix', save_path=None):\n",
    "        plt.figure()\n",
    "        xticks = class_names if class_names is not None else 'auto'\n",
    "        yticks = class_names if class_names is not None else 'auto'\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=xticks, yticklabels=yticks)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Prediction')\n",
    "        plt.ylabel('Real')\n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "def prepare_data_multiclass(\n",
    "    data_dir: str = TRAINING_URL,\n",
    "    image_size: int = 512,\n",
    "    seed: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga, redimensiona y etiqueta imágenes de las clases para clasificación binaria.\n",
    "    Devuelve (X, y) como arrays de numpy.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    class_map = CLASS_MAP\n",
    "    files_by_class = {}\n",
    "    for class_name in class_map:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.lower().endswith('.jpg')]\n",
    "        files_by_class[class_name] = files\n",
    "    X, y = [], []\n",
    "    for class_name, label in class_map.items():\n",
    "        for f in files_by_class[class_name]:\n",
    "            img = Image.open(f).convert('L').resize((image_size, image_size))\n",
    "            X.append(np.array(img))\n",
    "            y.append(label)\n",
    "    X = np.stack(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "class QuantumClassifier:\n",
    "    def __init__(self, n_qubits=16, pca_features=8, batch_size=16, epochs=20, lr=0.01, layers=3, seed=42):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.pca_features = pca_features\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Initialize preprocessing tools\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=self.pca_features)\n",
    "        self.scaler_angle = MinMaxScaler(feature_range=(0, np.pi / 2))\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self._prepare_data_custom()\n",
    "        self._build_model()\n",
    "\n",
    "    def _prepare_data_custom(self):\n",
    "        # Prepare training data and fit transformers\n",
    "        X_train_raw, self.y_train = prepare_data_multiclass(data_dir=TRAINING_URL, image_size=256, seed=self.seed)\n",
    "        X_train_flat = X_train_raw.reshape((X_train_raw.shape[0], -1)) / 255.0\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train_flat)\n",
    "        X_train_pca = self.pca.fit_transform(X_train_scaled)\n",
    "        self.x_train = self.scaler_angle.fit_transform(X_train_pca)\n",
    "\n",
    "        # Prepare testing data and apply the same transformations\n",
    "        X_test_raw, self.y_test = prepare_data_multiclass(data_dir=TESTING_URL, image_size=256, seed=self.seed)\n",
    "        X_test_flat = X_test_raw.reshape((X_test_raw.shape[0], -1)) / 255.0\n",
    "        X_test_scaled = self.scaler.transform(X_test_flat) # Use transform, not fit_transform\n",
    "        X_test_pca = self.pca.transform(X_test_scaled) # Use transform\n",
    "        self.x_test = self.scaler_angle.transform(X_test_pca) # Use transform\n",
    "\n",
    "    def _build_model(self):\n",
    "        dev = qml.device(\"lightning.qubit\", wires=self.n_qubits)\n",
    "        @qml.qnode(dev)\n",
    "        def qnode(inputs, weights):\n",
    "            aub = self.n_qubits // 2\n",
    "            qml.AngleEmbedding(inputs, wires=range(aub))\n",
    "            qml.BasicEntanglerLayers(weights, wires=range(aub))\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in range(aub)]\n",
    "        weight_shapes = {\"weights\": (self.layers, self.n_qubits // 2)}\n",
    "        \n",
    "        class HybridModel(nn.Module):\n",
    "            def __init__(self, input_features=16):\n",
    "                super().__init__()\n",
    "                self.input_features = input_features\n",
    "                self.clayer_1 = torch.nn.Linear(input_features, input_features)\n",
    "                self.qlayer_1 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "                self.qlayer_2 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "                self.clayer_2 = torch.nn.Linear(input_features, input_features)\n",
    "                self.final_layer = torch.nn.Linear(input_features, 1)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.clayer_1(x)\n",
    "                x_1, x_2 = torch.split(x, self.input_features//2, dim=1)\n",
    "                x_1 = self.qlayer_1(x_1)\n",
    "                x_2 = self.qlayer_2(x_2)\n",
    "                x = torch.cat([x_1, x_2], dim=1)\n",
    "                x = self.clayer_2(x)\n",
    "                x = self.final_layer(x)\n",
    "                return x\n",
    "\n",
    "        self.model = HybridModel(input_features=self.pca_features)\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        x_train_t = torch.tensor(self.x_train, dtype=torch.float32)\n",
    "        y_train_t = torch.tensor(self.y_train, dtype=torch.float32).unsqueeze(1)\n",
    "        train_loader = DataLoader(TensorDataset(x_train_t, y_train_t), batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        epoch_results = []\n",
    "        loss_history = []\n",
    "        accuracy_history = []\n",
    "        print(f\"--------------- EPOCHS --------------------\")\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(self.model(xb), yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            loss_history.append(avg_loss)\n",
    "            print(f\"Average loss over epoch {epoch + 1}: {avg_loss:.4f}\")\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                x_test_t = torch.tensor(self.x_test, dtype=torch.float32)\n",
    "                y_test_t = torch.tensor(self.y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                outputs = self.model(x_test_t)\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs >= 0.5).int()\n",
    "                correct = (preds == y_test_t.int()).sum().item()\n",
    "                acc = correct / len(y_test_t)\n",
    "                accuracy_history.append(acc)\n",
    "                y_true_np = y_test_t.numpy()\n",
    "                preds_np = preds.numpy()\n",
    "\n",
    "                f1 = f1_score(y_true_np, preds_np)\n",
    "                recall = recall_score(y_true_np, preds_np)\n",
    "                cm = confusion_matrix(y_true_np, preds_np)\n",
    "\n",
    "                print(f\"Accuracy: {acc:.4f} - F1: {f1:.4f} - Recall: {recall:.4f}\")\n",
    "                epoch_results.append((acc, f1, recall, epoch + 1, self.model.state_dict(), avg_loss, cm))\n",
    "                print(\"---------------------------------------------------\")\n",
    "        \n",
    "        epoch_results.sort(key=lambda x: x[0], reverse=True)\n",
    "        best_acc, best_f1, best_recall, best_ep, best_state_dict, best_loss, best_cm = epoch_results[0]\n",
    "\n",
    "        print(\"\\nSaving best model and preprocessing artifacts:\")\n",
    "        best_model_filename = self._save_model(best_state_dict, best_ep, best_acc, RESULTS_MODELS_URL)\n",
    "        \n",
    "        # Save the preprocessing artifacts corresponding to the best model\n",
    "        self._save_preprocessing_artifacts(best_model_filename, RESULTS_PREPROCESSING_URL)\n",
    "\n",
    "\n",
    "        PlotUtils.plot_loss(loss_history, save_path=os.path.join(RESULTS_GRAPHICS_URL, \"loss_plot.png\"))\n",
    "        PlotUtils.plot_confusion_matrix(best_cm, class_names=['Tumor', 'No Tumor'],\n",
    "                                        save_path=os.path.join(RESULTS_GRAPHICS_URL, \"confusion_matrix.png\"))\n",
    "        PlotUtils.plot_loss_vs_accuracy(\n",
    "            loss_history,\n",
    "            accuracy_history,\n",
    "            save_path=os.path.join(RESULTS_GRAPHICS_URL, \"loss_vs_accuracy.png\")\n",
    "        )\n",
    "        return {\n",
    "            'best_accuracy': best_acc,\n",
    "            'best_f1': best_f1,\n",
    "            'best_recall': best_recall,\n",
    "            'best_loss': best_loss,\n",
    "            'best_epoch': best_ep,\n",
    "            'confusion_matrix': best_cm.tolist(),\n",
    "            'model_filename': best_model_filename\n",
    "        }\n",
    "\n",
    "    def _save_model(self, model_state_dict, epoch, accuracy, save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = f\"BC_best_model_epoch_{epoch}_acc_{accuracy:.4f}.pt\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        torch.save(model_state_dict, filepath)\n",
    "        print(f\"Model saved: {filepath}\")\n",
    "        return filename\n",
    "    \n",
    "    def _save_preprocessing_artifacts(self, model_filename, save_dir):\n",
    "        \"\"\"\n",
    "        Saves the PCA and scaler objects to a specified directory.\n",
    "        The filenames are derived from the model filename to link them.\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        base_filename = os.path.splitext(model_filename)[0]\n",
    "\n",
    "        # Save PCA model\n",
    "        pca_path = os.path.join(save_dir, f\"{base_filename}_pca.pkl\")\n",
    "        joblib.dump(self.pca, pca_path)\n",
    "        print(f\"PCA model saved: {pca_path}\")\n",
    "\n",
    "        # Save StandardScaler\n",
    "        scaler_path = os.path.join(save_dir, f\"{base_filename}_scaler.pkl\")\n",
    "        joblib.dump(self.scaler, scaler_path)\n",
    "        print(f\"StandardScaler saved: {scaler_path}\")\n",
    "\n",
    "        # Save MinMaxScaler for angles\n",
    "        scaler_angle_path = os.path.join(save_dir, f\"{base_filename}_scaler_angle.pkl\")\n",
    "        joblib.dump(self.scaler_angle, scaler_angle_path)\n",
    "        print(f\"MinMaxScaler (angle) saved: {scaler_angle_path}\")\n",
    "\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self,n_qubits, epochs, lr, features, layers, batch_size, seed):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.features = features\n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "\n",
    "    def csv_log(self, results, csv_file):\n",
    "        duration_seconds = results.get('execution_time', None)\n",
    "        log_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        best_accuracy = results['best_accuracy']\n",
    "        best_epoch = results['best_epoch']\n",
    "        best_f1 = results.get('best_f1', '')\n",
    "        best_recall = results.get('best_recall', '')\n",
    "        loss = results.get('best_loss', '')\n",
    "        model_filename = results.get('model_filename', '')\n",
    "\n",
    "        header = [\n",
    "            'date', 'execution_time_seconds', 'epochs', 'learning_rate', 'features', 'layers', 'batch_size',\n",
    "            'loss', 'accuracy', 'recall', 'f1_score', 'epoch', 'model_filename'\n",
    "        ]\n",
    "        row = [\n",
    "            log_date,\n",
    "            f'{duration_seconds:.2f}' if duration_seconds is not None else 'no seconds',\n",
    "            self.epochs, self.lr, self.features, self.layers, self.batch_size,\n",
    "            loss, best_accuracy, best_recall, best_f1, best_epoch, model_filename\n",
    "        ]\n",
    "\n",
    "        file_exists = os.path.isfile(csv_file)\n",
    "        with open(csv_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if not file_exists:\n",
    "                writer.writerow(header)\n",
    "            writer.writerow(row)\n",
    "\n",
    "        print(f'Results saved to {csv_file}')\n",
    "        print('Run summary:')\n",
    "        print(row)\n",
    "\n",
    "    def log_class_distribution(self,unique_classes, counts, data_type):\n",
    "        print(f\"Distribution of classes in {data_type}:\")\n",
    "        for cls, count in zip(unique_classes, counts):\n",
    "            print(f\"  Class {cls}: {count} samples\")\n",
    "\n",
    "class QuantumRunner(ExperimentRunner):\n",
    "    def run(self):\n",
    "        print(\"\\n--- Running QUANTUM QuantumClassifier ---\")\n",
    "        qc = QuantumClassifier(\n",
    "            n_qubits=self.n_qubits,\n",
    "            pca_features=self.features,\n",
    "            batch_size=self.batch_size,\n",
    "            epochs=self.epochs,\n",
    "            lr=self.lr,\n",
    "            layers=self.layers,\n",
    "            seed=self.seed\n",
    "        )\n",
    "        unique_classes_train, counts_train = np.unique(qc.y_train, return_counts=True)\n",
    "        self.log_class_distribution(unique_classes_train, counts_train, \"training data (y_train)\")\n",
    "\n",
    "        unique_classes_test, counts_test = np.unique(qc.y_test, return_counts=True)\n",
    "        self.log_class_distribution(unique_classes_test, counts_test, \"test data (y_test)\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        results = qc.train_and_evaluate()\n",
    "        end_time = time.time()\n",
    "        self.duration = end_time - start_time\n",
    "        results['execution_time'] = self.duration\n",
    "        self.csv_log(results, os.path.join(RESULTS_CSV_URL, 'BC_results_log.csv'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # =====================\n",
    "    #### CONFIGURATION CONSTANTS\n",
    "    # =====================\n",
    "    EPOCHS = 200\n",
    "    LEARNING_RATE = 0.05\n",
    "    FEATURES = 12\n",
    "    N_QUBITS = FEATURES\n",
    "    LAYERS = 3\n",
    "    BATCH_SIZE = 64\n",
    "    SEED = 42\n",
    "\n",
    "    # Run the experiment\n",
    "    QuantumRunner(n_qubits=N_QUBITS, epochs=EPOCHS, lr=LEARNING_RATE, features=FEATURES, layers=LAYERS, batch_size=BATCH_SIZE, seed=SEED).run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
