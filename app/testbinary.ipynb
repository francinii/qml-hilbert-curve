{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146e9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, recall_score, f1_score, accuracy_score\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plot_utils import PlotUtils\n",
    "import csv\n",
    "from datetime import datetime\n",
    "#from pen import QuantumClassifier\n",
    "#from pen_hilbert import QuantumHilbertClassifier\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold # Importar KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a2f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data_multiclass(\n",
    "    data_dir: str = \"data/dataset_v2/Training/\",\n",
    "    image_size: int = 128,\n",
    "    seed: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga, redimensiona y etiqueta imágenes para clasificación binaria:\n",
    "    'tumor' (glioma, meningioma, pituitary) vs 'no_tumor'.\n",
    "    Realiza downsampling para balancear las clases (todas tendrán el mismo número de imágenes que la clase minoritaria).\n",
    "    Guarda un log de las imágenes seleccionadas en results/graphics/downsampling_log.txt.\n",
    "    Devuelve (X, y) como arrays de numpy.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # === CAMBIO CLAVE: Mapeo de clases para clasificación binaria ===\n",
    "    # Todas las categorías de tumores se mapean a la etiqueta 0 ('tumor')\n",
    "    # 'no_tumor' se mapea a la etiqueta 1 ('no_tumor')\n",
    "    raw_class_map = {\n",
    "        \"glioma_tumor\": 0,       # Ahora es 'tumor'\n",
    "        \"meningioma_tumor\": 0,   # Ahora es 'tumor'\n",
    "        \"pituitary_tumor\": 0,    # Ahora es 'tumor'\n",
    "        \"no_tumor\": 1            # Sigue siendo 'no_tumor'\n",
    "    }\n",
    "    \n",
    "    # Agrupar archivos por la nueva etiqueta binaria\n",
    "    files_by_binary_label = {0: [], 1: []} # 0 para 'tumor', 1 para 'no_tumor'\n",
    "\n",
    "    # Recopilar todos los archivos y asignar su etiqueta binaria\n",
    "    for class_name_raw, binary_label in raw_class_map.items():\n",
    "        class_dir = os.path.join(data_dir, class_name_raw)\n",
    "        \n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"Advertencia: El directorio {class_dir} no existe. Se saltará esta clase.\")\n",
    "            continue\n",
    "            \n",
    "        files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        files_by_binary_label[binary_label].extend(files)\n",
    "            \n",
    "    # === Downsampling para balancear las dos clases binarias (0 y 1) ===\n",
    "    # Encontrar el número de imágenes de la clase minoritaria\n",
    "    # Considera solo las clases que tienen imágenes\n",
    "    counts = [len(files) for files in files_by_binary_label.values() if len(files) > 0]\n",
    "    if not counts:\n",
    "        raise ValueError(f\"No se encontraron imágenes en ningún directorio de clases para generar datos en {data_dir}. Verifique la ruta y los contenidos.\")\n",
    "    \n",
    "    min_count_binary = min(counts)\n",
    "    print(f\"La clase binaria minoritaria tiene {min_count_binary} imágenes en {data_dir}\")\n",
    "    \n",
    "    selected_files_log = {} # Usaremos las etiquetas originales en el log para mayor claridad\n",
    "    final_X, final_y = [], []\n",
    "    \n",
    "    for binary_label, files_list in files_by_binary_label.items():\n",
    "        random.shuffle(files_list)\n",
    "        selected_for_binary_label = files_list[:min_count_binary] # Seleccionar hasta min_count_binary\n",
    "        \n",
    "        # Procesar imágenes y añadir al dataset final\n",
    "        for f in selected_for_binary_label:\n",
    "            try:\n",
    "                img = Image.open(f).convert('L').resize((image_size, image_size))\n",
    "                final_X.append(np.array(img))\n",
    "                final_y.append(binary_label)\n",
    "                \n",
    "                # Para el log, asociar el archivo con su nombre de clase original y la nueva etiqueta binaria\n",
    "                original_class_name = os.path.basename(os.path.dirname(f))\n",
    "                if original_class_name not in selected_files_log:\n",
    "                    selected_files_log[original_class_name] = []\n",
    "                selected_files_log[original_class_name].append(f)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al cargar la imagen {f}: {e}\")\n",
    "                continue # Saltar la imagen con error\n",
    "\n",
    "    # Guardar log (puede ser útil para depuración)\n",
    "    os.makedirs('results/logs', exist_ok=True)\n",
    "    log_file_path = os.path.join('results', 'logs', f'downsampling_log_{os.path.basename(os.path.normpath(data_dir))}.txt')\n",
    "    with open(log_file_path, 'w') as f:\n",
    "        f.write(f\"Downsampling log for data from: {data_dir}\\n\\n\")\n",
    "        f.write(\"Binary Class Mapping: 0='Tumor' (glioma, meningioma, pituitary), 1='No Tumor'\\n\\n\")\n",
    "        for original_class_name, files in selected_files_log.items():\n",
    "            # Determinar la etiqueta binaria para el log\n",
    "            current_binary_label = None\n",
    "            for k, v in raw_class_map.items():\n",
    "                if k == original_class_name:\n",
    "                    current_binary_label = v\n",
    "                    break\n",
    "            \n",
    "            f.write(f\"Original Class: {original_class_name} (Mapped to Binary Label: {current_binary_label}) - {len(files)} images selected:\\n\")\n",
    "            for file in files:\n",
    "                f.write(f\"    {file}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    final_X = np.stack(final_X)\n",
    "    final_y = np.array(final_y)\n",
    "    \n",
    "    # Imprimir la distribución final de las clases binarias\n",
    "    unique_labels, counts_labels = np.unique(final_y, return_counts=True)\n",
    "    print(f\"\\nDistribución final de clases binarias en {data_dir}:\")\n",
    "    for label, count in zip(unique_labels, counts_labels):\n",
    "        label_name = \"Tumor\" if label == 0 else \"No Tumor\"\n",
    "        print(f\"  Clase {label} ({label_name}): {count} imágenes\")\n",
    "\n",
    "    return final_X, final_y\n",
    "\n",
    "# NOTA: En tu clase QuantumClassifier, el n_classes debe cambiar a 2.\n",
    "# En la sección _build_model, la línea:\n",
    "# self.model = HybridModel(qlayer, n_classes=4, n_qubits_model=self.n_qubits, input_features=self.pca_features)\n",
    "# DEBE SER:\n",
    "# self.model = HybridModel(qlayer, n_classes=2, n_qubits_model=self.n_qubits, input_features=self.pca_features)\n",
    "\n",
    "# También, si estás usando PlotUtils, asegúrate de que plot_confusion_matrix use\n",
    "# los nuevos nombres de clases:\n",
    "# class_names = ['Tumor', 'No Tumor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42810b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuantumClassifier:\n",
    "    def __init__(self, n_qubits=8, pca_features=8, batch_size=16, epochs=20, lr=0.01, layers=3, seed=42):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.pca_features = pca_features\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self._prepare_data_custom()\n",
    "        self._build_model()\n",
    "\n",
    "    def _prepare_data_custom(self):\n",
    "        X, y_train = prepare_data_multiclass(data_dir=\"data/dataset_v2/Training/\", image_size=256, seed=self.seed)\n",
    "        X = X.reshape((X.shape[0], -1)) / 255.0  # flatten and normalize\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        pca = PCA(n_components=self.pca_features)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # Añadir este escalado\n",
    "        scaler_angle = MinMaxScaler(feature_range=(0, np.pi / 2))\n",
    "        X_train = scaler_angle.fit_transform(X_pca)\n",
    "\n",
    "        X_test, y_test = prepare_data_multiclass(data_dir=\"data/dataset_v2/Testing/\", image_size=256, seed=self.seed)\n",
    "        X_test = X_test.reshape((X_test.shape[0], -1)) / 255.0  # flatten and normalize\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled_test = scaler.fit_transform(X_test)\n",
    "        pca = PCA(n_components=self.pca_features)\n",
    "        X_pca_test = pca.fit_transform(X_scaled_test)\n",
    "        \n",
    "        # Añadir este escalado\n",
    "        scaler_angle = MinMaxScaler(feature_range=(0, np.pi / 2))\n",
    "        X_test = scaler_angle.fit_transform(X_pca_test)\n",
    "\n",
    "\n",
    "        # Usar X_pca_scaled para el split\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X_pca_scaled, y, test_size=0.2, random_state=self.seed)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = np.array(y_train)\n",
    "        self.y_test = np.array(y_test)\n",
    "\n",
    "    def _build_model(self):\n",
    "        #dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        dev = qml.device(\"lightning.qubit\", wires=self.n_qubits) # Sugerido\n",
    "        def circuit(inputs, weights):\n",
    "            #for i in range(self.n_qubits):\n",
    "            #   qml.RY(inputs[i], wires=i)\n",
    "            #qml.templates.StronglyEntanglingLayers(weights, wires=range(self.n_qubits))\n",
    "            qml.AngleEmbedding(inputs, wires=range(self.n_qubits), rotation='Y')\n",
    "            qml.AngleEmbedding(inputs, wires=range(self.n_qubits), rotation='Z')\n",
    "            # Usa un ansatz más simple y menos capas\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(self.n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "        #weight_shapes = {\"weights\": (self.layers, self.n_qubits, 3)}\n",
    "        weight_shapes = {\"weights\": (1, self.n_qubits)}  # <- CORRECTO\n",
    "\n",
    "        qlayer = qml.qnn.TorchLayer(qml.qnode(dev)(circuit), weight_shapes)\n",
    "\n",
    "        class HybridModel(nn.Module):\n",
    "            def __init__(self, qlayer, n_classes=2, n_qubits_model=None, input_features=None):\n",
    "                super().__init__()\n",
    "                self.fc_input = nn.Linear(input_features, n_qubits_model)  # <- corregido\n",
    "                self.qlayer = qlayer\n",
    "                self.bn1 = nn.BatchNorm1d(n_qubits_model)\n",
    "                self.hidden1 = nn.Linear(n_qubits_model, 64)\n",
    "                self.dropout1 = nn.Dropout(0.3)\n",
    "                self.hidden2 = nn.Linear(64, 32)\n",
    "                self.dropout2 = nn.Dropout(0.2)\n",
    "                self.output = nn.Linear(32, n_classes)\n",
    "                self.relu = nn.ReLU()\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.fc_input(x)\n",
    "                x = self.qlayer(x)\n",
    "                x = self.bn1(x)\n",
    "                x = self.relu(self.hidden1(x))\n",
    "                x = self.dropout1(x)\n",
    "                x = self.relu(self.hidden2(x))\n",
    "                x = self.dropout2(x)\n",
    "                return self.output(x)\n",
    "\n",
    "        self.model = HybridModel(qlayer, n_classes=2, n_qubits_model=self.n_qubits, input_features=self.pca_features)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        X_train_t = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        y_train_t = torch.tensor(self.y_train, dtype=torch.long)\n",
    "        train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=self.batch_size, shuffle=True)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "        loss_history = []\n",
    "        start = time.time()\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in train_loader:\n",
    "                pred = self.model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            loss_history.append(loss.item())\n",
    "            print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "                # <- Aquí va el scheduler\n",
    "            scheduler.step(loss.item())\n",
    "            for param_group in optimizer.param_groups:\n",
    "                print(\"Current LR:\", param_group['lr'])\n",
    "            \n",
    "        end = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X_test_t = torch.tensor(self.X_test, dtype=torch.float32)\n",
    "            y_test_t = torch.tensor(self.y_test, dtype=torch.long)\n",
    "            preds = self.model(X_test_t)\n",
    "            preds_cls = torch.argmax(preds, dim=1)\n",
    "            #probs = torch.softmax(preds, dim=1)\n",
    "            #threshold = 0.75  # Solo clasificamos como 'no_tumor' si hay > 90% de confianza\n",
    "\n",
    "            #preds_cls = torch.where(\n",
    "            #    probs[:, 1] >= threshold,  # índice 1 es 'no_tumor'\n",
    "            #    torch.tensor(1),\n",
    "            #    torch.tensor(0)\n",
    "            #)\n",
    "            acc = (preds_cls == y_test_t).float().mean().item()\n",
    "            cm = confusion_matrix(y_test_t.cpu().numpy(), preds_cls.cpu().numpy())\n",
    "            recall = recall_score(y_test_t.cpu().numpy(), preds_cls.cpu().numpy(), average='binary')\n",
    "            f1 = f1_score(y_test_t.cpu().numpy(), preds_cls.cpu().numpy(), average='binary')\n",
    "\n",
    "        class_names = ['tumor', 'no_tumor']\n",
    "        PlotUtils.plot_loss(loss_history, save_path='results/graphics/loss_function_pen_bin.png')\n",
    "        PlotUtils.plot_confusion_matrix(cm, class_names=class_names, save_path='results/graphics/confusion_matrix_pen_bin.png')\n",
    "\n",
    "        results = {\n",
    "            'epochs': self.epochs,\n",
    "            'learning_rate': self.lr,\n",
    "            'features': self.pca_features,\n",
    "            'layers': self.layers,\n",
    "            'batch_size': self.batch_size,\n",
    "            'loss': float(loss.item()),\n",
    "            'accuracy': acc,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'execution_time': end - start\n",
    "        }\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 score: {f1:.4f}\")\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc012d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    def __init__(self, epochs, lr, features, layers, batch_size, seed):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.features = features\n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "\n",
    "    def run_and_log(self, results, csv_file):\n",
    "        duration = results.get('execution_time', None)\n",
    "        date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        header = [\n",
    "            'date', 'execution_time', 'epochs', 'learning_rate', 'features', 'layers', 'batch_size',\n",
    "            'loss', 'accuracy', 'recall', 'f1_score'\n",
    "        ]\n",
    "        row = [\n",
    "            date, f'{duration:.2f}', results['epochs'], results['learning_rate'], results['features'],\n",
    "            results['layers'], results['batch_size'], results['loss'], results['accuracy'], results['recall'], results['f1_score']\n",
    "        ]\n",
    "        file_exists = os.path.isfile(csv_file)\n",
    "        with open(csv_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if not file_exists:\n",
    "                writer.writerow(header)\n",
    "            writer.writerow(row)\n",
    "        print('Results saved in', csv_file)\n",
    "        print('Run summary:')\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02037d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumRunner(ExperimentRunner):\n",
    "    def run(self):\n",
    "        print(\"\\n--- Running QUANTUM QuantumClassifier ---\")\n",
    "        qc = QuantumClassifier(\n",
    "            n_qubits=self.features,\n",
    "            pca_features=self.features,\n",
    "            batch_size=self.batch_size,\n",
    "            epochs=self.epochs,\n",
    "            lr=self.lr,\n",
    "            layers=self.layers,\n",
    "            seed=self.seed\n",
    "        )\n",
    "        unique_classes, counts = np.unique(qc.y_train, return_counts=True)\n",
    "        print(\"Distribution of classes in training data (y_train):\")\n",
    "        for cls, count in zip(unique_classes, counts):\n",
    "            print(f\"  Class {cls}: {count} samples\")\n",
    "        unique_classes_y, counts_y = np.unique(qc.y_test, return_counts=True)\n",
    "        print(\"Distribution of classes in training data (y_train):\")\n",
    "        for cls, counts_y in zip(unique_classes_y, counts_y):\n",
    "            print(f\"  Class {cls}: {counts_y} samples\")\n",
    "        start_time = time.time()\n",
    "        results = qc.train_and_evaluate()\n",
    "        end_time = time.time()\n",
    "        self.duration = end_time - start_time\n",
    "        results['execution_time'] = self.duration\n",
    "        self.run_and_log(results, 'results_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1714ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# CONFIGURATION CONSTANTS\n",
    "# =====================\n",
    "MODE = 'quantum'  # Options: 'quantum', 'quantum_hilbert', 'both'\n",
    "EPOCHS = 30\n",
    "# CONFIGURACIÓN SUGERIDA\n",
    "LEARNING_RATE = 0.0005 # O incluso 0.001\n",
    "FEATURES = 8\n",
    "LAYERS = 1\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "USE_HILBERT = True  # Only relevant for quantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b124f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running QUANTUM QuantumClassifier ---\n",
      "La clase binaria minoritaria tiene 2000 imágenes en data/dataset_v2/Training/\n",
      "\n",
      "Distribución final de clases binarias en data/dataset_v2/Training/:\n",
      "  Clase 0 (Tumor): 2000 imágenes\n",
      "  Clase 1 (No Tumor): 2000 imágenes\n",
      "La clase binaria minoritaria tiene 250 imágenes en data/dataset_v2/Testing/\n",
      "\n",
      "Distribución final de clases binarias en data/dataset_v2/Testing/:\n",
      "  Clase 0 (Tumor): 250 imágenes\n",
      "  Clase 1 (No Tumor): 250 imágenes\n",
      "Distribution of classes in training data (y_train):\n",
      "  Class 0: 2000 samples\n",
      "  Class 1: 2000 samples\n",
      "Distribution of classes in training data (y_train):\n",
      "  Class 0: 250 samples\n",
      "  Class 1: 250 samples\n",
      "Epoch 1: Loss = 0.7123\n",
      "Current LR: 0.0005\n",
      "Epoch 2: Loss = 0.5730\n",
      "Current LR: 0.0005\n",
      "Epoch 3: Loss = 0.7318\n",
      "Current LR: 0.0005\n",
      "Epoch 4: Loss = 0.6180\n",
      "Current LR: 0.0005\n",
      "Epoch 5: Loss = 0.5537\n",
      "Current LR: 0.0005\n",
      "Epoch 6: Loss = 0.6777\n",
      "Current LR: 0.0005\n",
      "Epoch 7: Loss = 0.6282\n",
      "Current LR: 0.0005\n",
      "Epoch 8: Loss = 0.4689\n",
      "Current LR: 0.0005\n",
      "Epoch 9: Loss = 0.6167\n",
      "Current LR: 0.0005\n",
      "Epoch 10: Loss = 0.5013\n",
      "Current LR: 0.0005\n",
      "Epoch 11: Loss = 0.6145\n",
      "Current LR: 0.0005\n",
      "Epoch 12: Loss = 0.5954\n",
      "Current LR: 0.0005\n",
      "Epoch 13: Loss = 0.4656\n",
      "Current LR: 0.0005\n",
      "Epoch 14: Loss = 0.5467\n",
      "Current LR: 0.0005\n",
      "Epoch 15: Loss = 0.4448\n",
      "Current LR: 0.0005\n",
      "Epoch 16: Loss = 0.6879\n",
      "Current LR: 0.0005\n",
      "Epoch 17: Loss = 0.6963\n",
      "Current LR: 0.0005\n",
      "Epoch 18: Loss = 0.5436\n",
      "Current LR: 0.0005\n",
      "Epoch 19: Loss = 0.6509\n",
      "Current LR: 0.0005\n",
      "Epoch 20: Loss = 0.8411\n",
      "Current LR: 0.0005\n",
      "Epoch 21: Loss = 0.4776\n",
      "Current LR: 0.00025\n",
      "Epoch 22: Loss = 0.4838\n",
      "Current LR: 0.00025\n",
      "Epoch 23: Loss = 0.5458\n",
      "Current LR: 0.00025\n",
      "Epoch 24: Loss = 0.4910\n",
      "Current LR: 0.00025\n",
      "Epoch 25: Loss = 0.4550\n",
      "Current LR: 0.00025\n",
      "Epoch 26: Loss = 0.5260\n",
      "Current LR: 0.00025\n",
      "Epoch 27: Loss = 0.6044\n",
      "Current LR: 0.000125\n",
      "Epoch 28: Loss = 0.5659\n",
      "Current LR: 0.000125\n",
      "Epoch 29: Loss = 0.5722\n",
      "Current LR: 0.000125\n",
      "Epoch 30: Loss = 0.3272\n",
      "Current LR: 0.000125\n",
      "Accuracy: 0.7280\n",
      "Confusion Matrix:\n",
      " [[195  55]\n",
      " [ 81 169]]\n",
      "Recall: 0.6760\n",
      "F1 score: 0.7131\n",
      "Results saved in results_log.csv\n",
      "Run summary:\n",
      "['2025-07-14 14:09:27', '195.78', 30, 0.0005, 8, 1, 32, 0.3271941542625427, 0.7279999852180481, 0.676, 0.7130801687763713]\n"
     ]
    }
   ],
   "source": [
    "QuantumRunner(EPOCHS, LEARNING_RATE, FEATURES, LAYERS, BATCH_SIZE, SEED).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
